{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: FrozenLakeNotSlippery-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f7485a9e0226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FrozenLakeNotSlippery-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gym.envs.toy_text:FrozenLakeEnv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'map_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'8x8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_slippery'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/ml_env/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dir_install/Aanconda3/anaconda3/envs/ml_env/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: FrozenLakeNotSlippery-v0"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "\n",
    "def running_mean(x, N=20):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.stateCnt = env.observation_space.n\n",
    "        self.actionCnt = env.action_space.n  # left:0; down:1; right:2; up:3\n",
    "        self.learning_rate = 0.01\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 0.1\n",
    "        self.Q = self._initialiseModel()\n",
    "\n",
    "    def _initialiseModel(self):\n",
    "        # init the Q_table for all (s, a) value is 0.5.\n",
    "        q_table = np.ones((self.stateCnt, self.actionCnt)) * 0.5\n",
    "        return q_table\n",
    "\n",
    "    def predict_value(self, s):\n",
    "\n",
    "        return self.Q[s, :]\n",
    "\n",
    "    def update_value_Q(self, s, a, r, s_next, done):\n",
    "        # update the Q-table\n",
    "        predict_Q = self.Q[s, a]\n",
    "        if done:\n",
    "            self.Q[s, a] += self.learning_rate * (r - predict_Q)\n",
    "        else:\n",
    "            self.Q[s, a] += self.learning_rate * (r + (self.gamma * np.max(self.Q[s_next, :])) - predict_Q)\n",
    "\n",
    "    def update_value_S(self, s, a, r, s_next, a_next, done):\n",
    "        # update the Q-table\n",
    "        predict_Q = self.Q[s, a]\n",
    "        if done:\n",
    "            self.Q[s, a] += self.learning_rate * (r - predict_Q)\n",
    "        else:\n",
    "            self.Q[s, a] += self.learning_rate * (r + (self.gamma * self.Q[s_next, a_next]) - predict_Q)\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.actionCnt)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[s, :])\n",
    "        return action\n",
    "\n",
    "    def updateEpsilon(self, episode_counter):\n",
    "        # update the Epsilon value smaller, exploitation -> exploration.\n",
    "        self.epsilon = self.epsilon / (1 + 0.1 * episode_counter)\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        print('Environment has %d states and %d actions.' % (self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.stateCnt = env.observation_space.n\n",
    "        self.actionCnt = env.action_space.n\n",
    "        self.maxStepsPerEpisode = 2500\n",
    "        self.q_Sinit_progress = np.array([[0.5, 0.5, 0.5, 0.5]])  # ex: np.array([[0,0,0,0]])\n",
    "\n",
    "    def run_episode_qlearning(self):\n",
    "        s = self.env.reset()  # \"reset\" environment to start state\n",
    "        r_total = 0\n",
    "        episodeStepsCnt = 0\n",
    "        success = False\n",
    "        for i in range(self.maxStepsPerEpisode):\n",
    "            if i == 0:\n",
    "                self.q_Sinit_progress = np.append(self.q_Sinit_progress, np.array([agent.Q[s]]), axis=0)\n",
    "            a = agent.choose_action(s)\n",
    "            s_next, r, done, info = self.env.step(a)\n",
    "            r_total += r\n",
    "            episodeStepsCnt += 1\n",
    "            # Update the Q-table value using Q-learning algorithm\n",
    "            agent.update_value_Q(s, a, r, s_next, done)\n",
    "            s = s_next\n",
    "            self.env.render()\n",
    "            if done:\n",
    "                if s_next == 63:\n",
    "                    success = True\n",
    "                break\n",
    "            # self.env.step(a): \"step\" will execute action \"a\" at the current agent state and move the agent to the\n",
    "            # nect state. step will return the next state, the reward, a boolean indicating if a terminal state is\n",
    "            # reached, and some diagnostic information useful for debugging. self.env.render(): \"render\" will print\n",
    "            # the current enviroment state. self.q_Sinit_progress = np.append( ): use q_Sinit_progress for monitoring\n",
    "            # the q value progress throughout training episodes for all available actions at the initial state.\n",
    "        return r_total, episodeStepsCnt, success\n",
    "\n",
    "    def run_episode_sarsa(self):\n",
    "        s = self.env.reset()  # \"reset\" environment to start state\n",
    "        r_total = 0\n",
    "        episodeStepsCnt = 0\n",
    "        success = False\n",
    "        a = agent.choose_action(s)\n",
    "        for i in range(self.maxStepsPerEpisode):\n",
    "            if i == 0:\n",
    "                self.q_Sinit_progress = np.append(self.q_Sinit_progress, np.array([agent.Q[s]]), axis=0)\n",
    "\n",
    "            s_next, r, done, info = self.env.step(a)\n",
    "            a_next = agent.choose_action(s_next)\n",
    "            r_total += r\n",
    "            episodeStepsCnt += 1\n",
    "            # Update the Q-table value using SARSA-learning algorithm\n",
    "            agent.update_value_S(s, a, r, s_next, a_next, done)\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "            self.env.render()\n",
    "            if done:\n",
    "                if s_next == 63:\n",
    "                    success = True\n",
    "                break\n",
    "            # self.env.step(a): \"step\" will execute action \"a\" at the current agent state and move the agent to the\n",
    "            # nect state. step will return the next state, the reward, a boolean indicating if a terminal state is\n",
    "            # reached, and some diagnostic information useful for debugging. self.env.render(): \"render\" will print\n",
    "            # the current enviroment state. self.q_Sinit_progress = np.append( ): use q_Sinit_progress for monitoring\n",
    "            # the q value progress throughout training episodes for all available actions at the initial state\n",
    "        return r_total, episodeStepsCnt, success\n",
    "\n",
    "    def run_evaluation_episode(self):\n",
    "        agent.epsilon = 0\n",
    "        self.run_episode_qlearning()\n",
    "        # self.run_episode_sarsa()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "    world = World(env)\n",
    "    agent = Agent(env)  # This will creat an agent\n",
    "    r_total_progress = []\n",
    "    episodeStepsCnt_progress = []\n",
    "    nbOfTrainingEpisodes = 1200\n",
    "    for i in range(nbOfTrainingEpisodes):\n",
    "        print('\\n========================\\n   Episode: {}\\n========================'.format(i))\n",
    "        # run_episode_qlearning or run_episode_sarsa\n",
    "        r_total, episodeStepsCnt, success = world.run_episode_qlearning()\n",
    "        # r_total, episodeStepsCnt, success = world.run_episode_sarsa()\n",
    "\n",
    "        # append to r_total_progress and episodeStepsCnt_progress\n",
    "        episodeStepsCnt_progress.append(episodeStepsCnt)\n",
    "        r_total_progress.append(r_total)\n",
    "        agent.updateEpsilon(i)\n",
    "\n",
    "    # run_evaluation_episode\n",
    "    print('=============================================================')\n",
    "    world.run_evaluation_episode()\n",
    "\n",
    "\n",
    "    ### --- Plots --- ###\n",
    "    # 1) plot world.q_Sinit_progress\n",
    "    fig1 = plt.figure(1)\n",
    "    plt.ion()\n",
    "    plt.plot(world.q_Sinit_progress[:, 0], label='left', color='r')\n",
    "    plt.plot(world.q_Sinit_progress[:, 1], label='down', color='g')\n",
    "    plt.plot(world.q_Sinit_progress[:, 2], label='right', color='b')\n",
    "    plt.plot(world.q_Sinit_progress[:, 3], label='up', color='y')\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size('small')\n",
    "    plt.legend(prop=fontP, loc=1)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    # 2) plot the evolution of the number of steps per successful episode throughout training. A successful episode\n",
    "    # is an episode where the agent reached the goal (i.e. not any terminal state)\n",
    "    fig2 = plt.figure(2)\n",
    "    plt1 = plt.subplot(1, 2, 1)\n",
    "    plt1.set_title(\"Number of steps per successful episode\")\n",
    "    plt.ion()\n",
    "    plt.plot(episodeStepsCnt_progress)\n",
    "    plt.pause(0.0001)\n",
    "    # 3) plot the evolution of the total collected rewards per episode throughout training. you can use the\n",
    "    # running_mean function to smooth the plot\n",
    "    plt2 = plt.subplot(1, 2, 2)\n",
    "    plt2.set_title(\"Rewards collected per episode\")\n",
    "    plt.ion()\n",
    "    r_total_progress = running_mean(r_total_progress)\n",
    "    plt.plot(r_total_progress)\n",
    "    plt.pause(0.0001)\n",
    "    ### --- ///// --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
