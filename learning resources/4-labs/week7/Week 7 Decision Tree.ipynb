{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This week lab is about decision tree algorithm. We will use scikit-learn ML Pythpn library for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install scikit-learn: pip install -U scikit-learn\n",
    "# For other installation ways refer to: https://scikit-learn.org/stable/install.html\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) is a class capable of performing multi-class classification on a dataset using the decison tree algorithm. \n",
    "\n",
    "`DecisionTreeClassifier` takes as input two arrays: an array X, sparse or dense, of size `[n_samples, n_features]` holding the training samples, and an array Y of integer values, size `[n_samples]`, holding the class labels for the training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with really simple example\n",
    "X = [[0, 0], [1, 1]] # input \n",
    "Y = [0, 1] # target\n",
    "# Define the decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# train/fit the classifer on our data\n",
    "clf = clf.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After being fitted, the model can then be used to predict the class of samples:\n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n",
    "clf.predict_proba([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's train the decision tree classfier on a real dataset called the iris data. \n",
    "####  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. \n",
    "\n",
    "\n",
    "There are four features\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "And the three classes are\n",
    "\n",
    "0. Iris Setosa\n",
    "1. Iris Versicolour\n",
    "2. Iris Virginica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn has the iris data \n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# loading the data \n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# let's see some samples\n",
    "# Inputs \n",
    "print(X[0::50])\n",
    "# and their targets\n",
    "print(y[0::50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we define the classifier, we would like to know which crtieron to use \n",
    "# to measure the quality of a split. We know there are two criteria:\n",
    "# “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "# The first one is used in the CART algorithm and the second is used in \n",
    "# the ID3 algorithm\n",
    "\n",
    "# To choose which algorithm to use in the decision tree algorithm we can just \n",
    "# the criterion argument we pass to DecisionTreeClassifier class to \"gini\" for CART algorithm \n",
    "# and \"entropy\" for the ID3 algorithm\n",
    "\n",
    "\n",
    "# For now we will use the ID3 algorithm \n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "# fit the model on our data\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's show the constructed tree \n",
    "plt.figure(dpi=200)\n",
    "tree.plot_tree(clf, filled=True, feature_names=['sepal length', 'sepal width','petal length','petal width '],\n",
    "               class_names=['Setosa', 'Versicolour', 'Virginica'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Build the decision tree classifier on the same dataset using the CART algorithm \n",
    "# and show the constructed tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classfier using CART algorithm\n",
    "cart_clf = None\n",
    "# fit the model on our data\n",
    "cart_clf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's show the constructed tree \n",
    "plt.figure(dpi=200)\n",
    "tree.plot_tree(cart_clf, filled=True, feature_names=['sepal length', 'sepal width','petal length','petal width '],\n",
    "               class_names=['Setosa', 'Versicolour', 'Virginica'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there any different between the two the constructed trees ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare between the two algorithm with spliting the data into train set and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets shuffle and split the data\n",
    "\n",
    "# Randomly order the data\n",
    "order = list(range(np.shape(X)[0]))\n",
    "np.random.shuffle(order)\n",
    "X = X[order,:]\n",
    "Y = y[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "train = X[:120]\n",
    "traint = y[:120]\n",
    "test = X[120:]\n",
    "testt = y[120:]\n",
    "print(train.shape, traint.shape, test.shape, testt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the decision tree classifier using both algorithms (ID3 and CART) on the train set \n",
    "# and compute the test accuracy of both classifiers on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: train the classifier using ID3 \n",
    "\n",
    "\n",
    "clf_id3 = None\n",
    "# fit the model on our data\n",
    "clf_id3 = None\n",
    "# test \n",
    "y_pred = None\n",
    "# accuracy\n",
    "accuracy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: train the classifier using CART \n",
    "\n",
    "clf_cart = None\n",
    "# fit the model on our data\n",
    "clf_cart = None\n",
    "# test \n",
    "y_pred = None\n",
    "# accuracy\n",
    "accuracy = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
