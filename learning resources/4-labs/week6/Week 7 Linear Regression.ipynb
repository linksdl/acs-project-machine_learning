{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this lab, we will implement Linear Regression using Least-square Solution. We will use the same example as we did in the class (Slide 18 from the linear regression slides). There are 5 steps. Let's implement them using only numpy step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linreg_steps](linreg_slide_18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the dataset: {(0,0), (0,1), (1,0)} and asked to find the \\\n",
    "least-squares solution for the parameters in the regression of \\\n",
    "the function: y = w1 +w2^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the input and target in numpy arrays \n",
    "inputs = np.array([[0], [0], [1]])\n",
    "targets = np.array([[0], [1], [0]])\n",
    "print('inputs shape :',np.shape(inputs))\n",
    "print('targets shape :',np.shape(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do the steps to find the solution  \n",
    "# Step 1: evaluate the basis on the points\n",
    "inputs = np.concatenate((np.ones((np.shape(inputs)[0],1)),inputs),axis=1)\n",
    "print('inputs shape :',np.shape(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: compute -> transpose(inputs) * inputs \n",
    "q_matrix = np.dot(np.transpose(inputs),inputs)\n",
    "print('q_matrix shape :',np.shape(q_matrix))\n",
    "print(q_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: invert q_matrix\n",
    "q_inverse = np.linalg.inv(q_matrix)\n",
    "print('q_inverse shape :',np.shape(q_inverse))\n",
    "print(q_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: Compute the pseudo-inverse -> q_inverse * transpose(inputs)\n",
    "q_pseudo = np.dot(q_inverse,np.transpose(inputs))\n",
    "print('q_pseudo shape :',np.shape(q_pseudo))\n",
    "print(q_pseudo.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: compute w = q_pseudo * targets\n",
    "weights = np.dot(q_pseudo,targets)\n",
    "print('w shape :',np.shape(weights))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's implement the steps but on a real dataset. we will work on the auto-mpg dataset. This consists of a collection of a number of datapoints about certain cars (weight, horsepower, etc.), with the aim being to predict the fuel efficiency in miles per gallon (mpg) in for each car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are asked to\n",
    "    - load the dataset text file (auto-mpg.txt) as numpy array \n",
    "    - prerocess the dataset (normalise, split it into train and test sets)\n",
    "    - find the least-squares solution for the parameters (weights vector)\n",
    "    - test the found parameters on the test set and calculate the error\n",
    "\n",
    "The following comments and codes are meant to guide you. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Please note: This dataset has one problem. There are missing values \n",
    "in it (labelled with question marks ‘?’). The np.loadtxt() method doesn’t\n",
    "like these, and we don’t know what to do with them, anyway,manually edit \n",
    "the file and delete all lines where there is a ? in that line. The linear\n",
    "regressor can’t do much with the names of the cars either, but since they \n",
    "appear in quotes(\") we will tell np.loadtxt that they are comments\n",
    "\n",
    "\n",
    "Below are the attribute Information for the dataset:\n",
    "\n",
    "    1. mpg:           continuous \n",
    "    2. cylinders:     multi-valued discrete\n",
    "    3. displacement:  continuous\n",
    "    4. horsepower:    continuous\n",
    "    5. weight:        continuous\n",
    "    6. acceleration:  continuous\n",
    "    7. model year:    multi-valued discrete\n",
    "    8. origin:        multi-valued discrete\n",
    "    9. car name:      string (unique for each instance)\n",
    "\n",
    "Please note: the first column is our target (mpg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the dataset file using np.loadtxt()\n",
    "\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalise the dataset. You can do this easily in numpy \n",
    "# by using np.mean and np.var. The only place where care is needed \n",
    "# is along which axis the mean and variance are computed: \n",
    "# axis=0 sums down the columns and axis=1 sums across the rows.\n",
    "\n",
    "normalised_date = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now separate the data into training and testing sets,\n",
    "\n",
    "training, testing = None, None \n",
    "\n",
    "# And split each set into inputs and targets hint: slicing the array\n",
    "trainin, traintgt = None, None\n",
    "testin, testtgt = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the training set to find the weights vector.\n",
    "# you need to implement the previous 5 steps on the training set \n",
    "# and find the weights vector (this is called training).  \n",
    "# To make it simple we define a function that takes \n",
    "# two args: inputs and targets and return the weights vector\n",
    "\n",
    "def linreg(inputs,targets):\n",
    "    # you should implement the 5 steps here\n",
    "    \n",
    "    weights = None\n",
    "\n",
    "    \n",
    "    return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your implementation \n",
    "weights = linreg(trainin,traintgt) \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testing the found weights on the testing set \n",
    "# you can do this by \n",
    "#     - testout = (testin*weights)\n",
    "#     - error = sum((testout - testtgt)**2)\n",
    "\n",
    "testout = None\n",
    "error = None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    You can try to re-train the model without the normalising the data \n",
    "    and see if this makes any different on the error value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
